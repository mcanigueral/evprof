---
title: "Profiling EV users with {evprof}"
# subtitle: "The Arnhem case study"
output: html_document
  # rmdformats::downcute:
  #     code_folding: show # To have the option to hide/show the code
  #     includes:
  #       in_header: www/header.html
  #       # before_body: www/header.html # This causes problems in zooming images
  #       after_body: www/footer.html
  #     self_contained: false # To not show a warning message from MathJax
  #     thumbnails: false # To show the images in their full-size
  #     lightbox: true # To make zoom to images
  #     gallery: true # To navigate from an image to other
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, error = F, fig.width = 10)

library(evprof)
library(dplyr)
library(lubridate)
library(ggplot2)
library(dygraphs)
library(purrr)
library(tidyr)
library(dutils)

options(
  evprof.tzone = "Europe/Amsterdam",
  evprof.start.time = 6
)

# Data
load('arnhem_data/arnhem_sessions_profiles.RData')
load('arnhem_data/arnhem_sessions_divided.RData')
load('arnhem_data/arnhem_sessions_models.RData')
load('arnhem_data/arnhem_comparison_demand.RData')
```


# Introduction

This article shows the methodology used to classify EV charging sessions among generic user profiles, using the R package `{evprof}`. The goal of this analysis is to define generic **user profiles** or, in other words, **EV connection patterns** that could relate a charging sessions to a specific user behaviour. 
The most relevant variables of the data set are the *Connection Start Time* and the *Connection Hours (duration)*.Based on these two variables `{evprof}` package aims to provide tools to analyze, cluster and model different user profiles with similar flexibility potential to, in a farther stage, exploit their flexibility in specific demand response programs.

The work-flow followed to perform this analysis is the following one:

1. Data exploratory analysis
    i) Data set visualization
    ii) Statistic analysis
2. Pre-processing
    i) Data division
    ii) Outliers cleaning
3. Clustering
4. Profiling
5. Classification
6. Modeling
7. Comparison with real data

This article applies the methodology proposed to a real sessions data set from the city of Arnhem. However, the aim of package `{evprof}` is to replicate the same methodology to other real data sets for any other country in the world. If you use this package don't hesitate to comment your results with the authors of `{evprof}`.


# Data exploratory analysis

## Data set visualization

The Arnhem's charging sessions data set contains `r nrow(sessions)` sessions, from `r min(sessions$ConnectionStartDateTime)` to `r max(sessions$ConnectionStartDateTime)`.

```{r dataset}
knitr::kable(head(sessions))
```

The following plot represents with a point every charging session in hour data set, based on the two most relevant features to define a user profile: Connection Start Time and Connection Hours (duration).

```{r exploration-points}
plot_points(sessions, start = 6, log = F, size = 0.25)
```

## Statistic analysis

The average values of the most important features in our charging sessions data set are:

```{r statistic summary}
summarise_sessions(sessions, mean) %>% knitr::kable(digits = 2)
```

Even though average values don't give a clear view of the sessions, in general our data set has:

* Low charging power (new EV models can charge up to 22 kW AC), probably most of sessions charge with 3.7 or 7.3 kW rates.
* Low energy demand (most of models have at least 40 kWh of battery), probably due to the energy spent in the travel home-city.
* Long connections, probably during the night.
* Low charging hours due to low energy demand.
* High flexibility due to long connections and low energy demand.

However, the goal of analyzing different user profiles is to find groups of sessions with different features averages. A more depth overview of the data set in this sense can be done with a distribution plot (histogram) of each one of these features:

```{r histograms}
plot_histogram_grid(sessions)
```
There is no feature that fits a Gaussian distribution, and some features like `ConnectionHours` have more than one peak in the distribution curve. This shows the existence and combination of different EV user profiles with independent distributions.


# Data preprocessing

The clustering method used in package `{evprof}` is Gaussian Mixture Models with Expectation-Maximization algorithm, wrapping functions from `{mclust}` package. To obtain a better performance in GMM clustering it is important to divide the data in smaller groups and clean the outliers, since the different density distributions will result more accentuated and easy to model.

## Divide the data

The division is performed in two steps:

1. Disconnection day
2. Time-cycle behaviors

### Disconnection day division

The different data points groups that stand out from overview plot correspond to sessions disconnection day. With function `plot_division_lines` we can visualize a division from a certain hour. We will set the division line from 3:00 AM, which means that sessions below division line disconnect before 3:00 AM of that day, each line corresponding to a different day.

```{r division lines}
plot_points(sessions, size = 0.25) %>% 
  plot_division_lines(n_lines = 4, division_hour = 3)
```

After finding the proper hour to make the division, function `divide_by_disconnection()` makes this division adding an extra column to the data set, `Disconnection`, with the number of the corresponding disconnection day. The sessions distribution over these 5 disconnection days is:

```{r divide by disconnection day}
sessions_divisions <- sessions %>% 
  divide_by_disconnection(days = 1:5, division_hour = 3)
```

```{r distribution disconnection day, echo = FALSE}
sessions_divisions %>% 
  group_by(Disconnection) %>% 
  summarise(n = n()) %>% 
  mutate(pct = n/sum(n)*100) %>% 
  knitr::kable(digits = 2, col.names = c("Disconnection day", "Number of sessions", "Percentage of sessions (%)"))
```

Almost all sessions disconnect the same day or the day after the connection since only 3% of sessions disconnect 2, 3 or 4 days after the connection. 

### Time-cycle division

It is also important to consider the time-cycles or periods when users change their behaviors. The function `plot_density_2D` lets to analyze the different density of sessions (i.e. users behavior) according to different weekday, month or year.

```{r density 2D plot wday}
plot_density_2D(sessions_divided, by = 'wday')
```
```{r density 2D plot month}
plot_density_2D(sessions_divided, by = 'month')
```

While depending on the weekday the density of sessions is highly diverse, between months we can't see a big difference. Moreover, we could see different distributions on Monday-Thursday, Friday, Saturday and Sunday, thus we will consider four different groups: working days (Monday - Thursday), Fridays, Saturdays, Sundays. The division for time-cycles sessions is performed by function `divide_by_timecycle()`, which adds an extra column `Timecycle` to the sessions data set with the number of time-cycle according to function parameters `months_cycles` and `wdays_cycles`.

```{r divide by time cycle}
sessions_divisions <- sessions_divisions %>% 
  divide_by_timecycle(months_cycles = list(1:12), wdays_cycles = list(1:4, 5, 6, 7))
```

```{r distribution time cycle, echo = FALSE}
sessions_divisions %>% 
  group_by(Timecycle) %>% 
  summarise(n = n()) %>% 
  mutate(pct = n/sum(n)*100) %>% 
  knitr::kable(digits = 2, col.names = c("Time-cycle", "Number of sessions", "Percentage of sessions (%)"))
```

### Divided data set

Once the two division approaches are applied, our data set counts with two extra columns (`Disconnection` and `Timecycle`) to classify each data point within each group. These columns have integer values corresponding to each division. For a more readable data set we can change these integer values by character strings with the definition of each group, and convert them to `factors` to set a specific order of the levels.

However, in the Disconnection day division we have seen that sessions disconnecting 2, 3 or 4 days after the connection represent just a 3% of the total data set. Thus, for obtaining generic user profiles these sessions will be discarded. The two *Disconnection* groups that we will analyze are the sessions that disconnect during the same connection day, labeled as **City sessions**, and the sessions that disconnect the day after connection, named **Home sessions**. At the same time, each *Time-cycle* group has the name of the corresponding weekday: **Workday**, **Friday**, **Saturday** and **Sunday**.

```{r divisions, eval=F}
sessions_divided <- sessions_divisions %>% 
  filter(Disconnection %in% c("1", "2")) %>%
  mutate(
    Disconnection = plyr::mapvalues(Disconnection, c("1", "2"), c("City", "Home")),
    Disconnection = factor(Disconnection, levels = c("City", "Home")),
    Timecycle = plyr::mapvalues(Timecycle, c("1", "2", "3", "4"), c("Workday", "Friday", "Saturday", "Sunday")),
    Timecycle = factor(Timecycle, levels = c("Workday", "Friday", "Saturday", "Sunday"))
  )

head(sessions_divided)
```

```{r, eval=F, echo=F}
save(sessions, sessions_divisions, sessions_divided, file = 'arnhem_sessions_divided.RData')
```

## Outliers cleaning

As explained, the clustering method used in package `{evprof}` is Gaussian Mixture Models clustering. This method is sensible to outliers since it tries to explain as most as possible all the variance of the data. This results to wide and low-precision Gaussian distributions (clusters). Therefore `{evprof}` package provides different functions to detect and filter outliers. At the same time, it is also recommended to perform the clustering process in a logarithmic scale, to include negative values to originally positive variables. The logarithmic transformation can be done in a lot of functions seting the `log` parameter to `TRUE`. We can visualise the data of each subset:

```{r divisions density plot}
plot_points(sessions_divided, size = 0.2, log = T) + facet_wrap(vars(Timecycle, Disconnection), scales = 'free', ncol = 4)
```
```{r divisions density plot save, eval=F, echo=F}
ggsave('figures/divisions.pdf', paper="a4r", width = 40, height = 20)
```

In these plots we see that every group has several points that stand out from most of points. These outliers can be detected specifying a **noise threshold** (in percentage) in function `detect_outliers()`. This function adds a logical column `Outlier` to the sessions data set showing if a session whether a session is considered outlier. This classification can be visualized with function `plot_outliers()` which shows the outliers in grey and the noise level (in percentage) in the title of the graph. This `Outlier` extra column can be removed together with outliers with function `drop_outliers()`. Additionally, to simply discard sessions from a certain limit in both axis (i.e connection hours or starting hour), function `cut_sessions()` filters the sessions data set according to the specified minimum and maximum limits of the corresponding axis.

The following plots show the noise-detection process performed for the Arnhem's data set and the corresponding filtering, resulting in 8 different clean sub sets ready to be clustered. 

### Workday city sessions

```{r, eval=F}
swc <- sessions_divided %>% 
  filter(Disconnection == "City", Timecycle == "Workday") %>% 
  cut_sessions(connection_start_min = 1.85, connection_start_max = 3.15, log=T) %>% 
  detect_outliers(MinPts = 100, noise_th = 2, log = T)
```
```{r}
swc %>% plot_outliers(log = T, size = 0.25)
```
```{r}
sessions_workday_city <- swc %>% 
  drop_outliers()
```

### Workday home sessions

```{r, eval=F}
swh <- sessions_divided %>% 
  filter(Disconnection == "Home", Timecycle == "Workday") %>% 
  cut_sessions(connection_hours_min = 1.5, log=T) %>% 
  detect_outliers(MinPts = 200, noise_th = 2, log = T)
```
```{r}
swh %>% plot_outliers(log = T, size = 0.25)
```
```{r}
sessions_workday_home <- swh %>% 
  drop_outliers()
```

### Friday city sessions

```{r, eval=F}
sfc <- sessions_divided %>% 
  filter(Disconnection == "City", Timecycle == "Friday") %>% 
  detect_outliers(MinPts = 50, noise_th = 2, log = T)
```
```{r}
sfc %>% plot_outliers(log = T, size = 0.25)
```
```{r}
sessions_friday_city <- sfc %>% 
  drop_outliers()
```

### Friday home sessions

```{r, eval=F}
sfh <- sessions_divided %>% 
  filter(Disconnection == "Home", Timecycle == "Friday") %>% 
  cut_sessions(connection_hours_min = 1.5, connection_start_min = 2.5, log=T) %>% 
  detect_outliers(MinPts = 200, noise_th = 5, log = T)
```
```{r}
sfh %>% plot_outliers(log = T, size = 0.25)
```
```{r}
sessions_friday_home <- sfh %>% 
  drop_outliers()
```

### Saturday city sessions

```{r, eval=F}
ssac <- sessions_divided %>% 
  filter(Disconnection == "City", Timecycle == "Saturday") %>% 
  detect_outliers(MinPts = 200, noise_th = 5, log = T)
```
```{r}
ssac %>% plot_outliers(log = T, size = 0.25)
```
```{r}
sessions_saturday_city <- ssac %>% 
  drop_outliers()
```

### Saturday home sessions

```{r, eval=F}
ssah <- sessions_divided %>% 
  filter(Disconnection == "Home", Timecycle == "Saturday") %>% 
  cut_sessions(connection_hours_min = 1.5, log=T) %>% 
  detect_outliers(MinPts = 50, noise_th = 6, log = T)
```
```{r}
ssah %>% plot_outliers(log = T, size = 0.25)
```
```{r}
sessions_saturday_home <- ssah %>% 
  drop_outliers()
```

### Sunday city sessions

```{r, eval=F}
ssuc <- sessions_divided %>% 
  filter(Disconnection == "City", Timecycle == "Sunday") %>% 
  cut_sessions(connection_start_min = 2.2, log = T ) %>% 
  detect_outliers(MinPts = 50, noise_th = 6, log = T)
```
```{r}
ssuc %>% plot_outliers(log = T, size = 0.25)
```
```{r}
sessions_sunday_city <- ssuc %>% 
  drop_outliers()
```

### Sunday home sessions

```{r, eval=F}
ssuh <- sessions_divided %>% 
  filter(Disconnection == "Home", Timecycle == "Sunday") %>% 
  cut_sessions(connection_hours_min = 1.5, connection_start_min = 2.3, connection_start_max = 3.25, log = T) %>% 
  detect_outliers(MinPts = 200, noise_th = 5, log = T)
```
```{r}
ssuh %>% plot_outliers(log = T, size = 0.25)
```
```{r}
sessions_sunday_home <- ssuh %>% 
  drop_outliers()
```

```{r, eval=F, echo=F}
save.image('arnhem_sessions_clean.RData')
```


# Clustering process

Function `cluster_sessions` perform a classification of sessions adding an extra column `Cluster` with the corresponding cluster number. However, Gaussian Mixture Models need a predefined number of clusters `k`. Moreover, this function also requires a `seed` in order to define a specific random seed and being able to reproduce specifics clustering results.

## Parameters selection

The Bayesan Information Criterion (BIC) is a common approach to find the optimal number of clusters to consider for the GMM clustering. The BIC penalizes the non-explained variability by the clusters found, so the lower the BIC value is the better the model will be. Function `choose_k_GMM()` generates a BIC plot wrapped from `{mclust}` package.

```{r number of clusters, eval=FALSE, echo=FALSE}
pdf('figures/BIC/BIC_sessions_workday_city.pdf', paper = 'a4r', width = 10, height = 7)
choose_k_GMM(sessions_workday_city, k = 3:10, log = T)
dev.off()
pdf('figures/BIC/BIC_sessions_workday_home.pdf', paper = 'a4r', width = 10, height = 7)
choose_k_GMM(sessions_workday_home, k = 3:10, log = T)
dev.off()

pdf('figures/BIC/BIC_sessions_friday_city.pdf', paper = 'a4r', width = 10, height = 7)
choose_k_GMM(sessions_friday_city, k = 3:10, log = T)
dev.off()
pdf('figures/BIC/BIC_sessions_friday_home.pdf', paper = 'a4r', width = 10, height = 7)
choose_k_GMM(sessions_friday_home, k = 3:10, log = T)
dev.off()

pdf('figures/BIC/BIC_sessions_saturday_city.pdf', paper = 'a4r', width = 10, height = 7)
choose_k_GMM(sessions_saturday_city, k = 3:10, log = T)
dev.off()
pdf('figures/BIC/BIC_sessions_saturday_home.pdf', paper = 'a4r', width = 10, height = 7)
choose_k_GMM(sessions_saturday_home, k = 3:10, log = T)
dev.off()

pdf('figures/BIC/BIC_sessions_sunday_city.pdf', paper = 'a4r', width = 10, height = 7)
choose_k_GMM(sessions_sunday_city, k = 3:10, log = T)
dev.off()
pdf('figures/BIC/BIC_sessions_sunday_home.pdf', paper = 'a4r', width = 10, height = 7)
choose_k_GMM(sessions_sunday_home, k = 3:10, log = T)
dev.off()
```

After generating a BIC plot for each one of the 8 sub-sets, the selected number of clusters are:

- Sessions workday city: `k = 6`
- Sessions workday home: `k = 6`
- Sessions friday city: `k = 6`
- Sessions friday home: `k = 6`
- Sessions saturday city: `k = 5`
- Sessions saturday home: `k = 5`
- Sessions sunday city: `k = 5`
- Sessions sunday home: `k = 6`


Then, since Gaussian Mixture Modeling depends on the random seed of the operation, it is recommended to repeat the clustering process several times and observe the variability. If different clusters are obtained in each repetition, it means that there is still too much noise or variance in the data, or that a different number of clusters should be selected. Function `save_clustering_iterations()` repeats the clustering the number of times specified in the `it` parameter and saves the results in a PDF file. From this file, then we can choose the optimal seed according to the BIC value. 

```{r clustering iteration, eval=FALSE, echo=FALSE}
save_clustering_iterations(sessions_workday_city, k=6, it=6, filename = "figures/CLUSTERS/workday_city_k-6.pdf", log = T)
save_clustering_iterations(sessions_workday_home, k=6, it=6, filename = "figures/CLUSTERS/workday_home_k-6.pdf", log = T)
save_clustering_iterations(sessions_friday_city, k=6, it=6, filename = "figures/CLUSTERS/friday_city_k-6_2.pdf", log = T)
save_clustering_iterations(sessions_friday_home, k=6, it=6, filename = "figures/CLUSTERS/friday_home_k-6_2.pdf", log = T)
save_clustering_iterations(sessions_saturday_city, k=5, it=6, filename = "figures/CLUSTERS/saturday_city_k-5.pdf", log = T)
save_clustering_iterations(sessions_saturday_home, k=5, it=6, filename = "figures/CLUSTERS/saturday_home_k-5_2.pdf", log = T)
save_clustering_iterations(sessions_sunday_city, k=5, it=6, filename = "figures/CLUSTERS/sunday_city_k-5.pdf", log = T)
save_clustering_iterations(sessions_sunday_home, k=5, it=6, filename = "figures/CLUSTERS/sunday_home_k-5.pdf", log = T)

save_clustering_iterations(sessions_workday_city, k=8, it=6, filename = "figures/CLUSTERS/workday_city_k-8.pdf", log = T)
save_clustering_iterations(sessions_workday_city, k=10, it=6, filename = "figures/CLUSTERS/workday_city_k-10.pdf", log = T)
save_clustering_iterations(sessions_workday_home, k=7, it=6, filename = "figures/CLUSTERS/workday_home_k-7.pdf", log = T)
save_clustering_iterations(sessions_friday_home, k=7, it=6, filename = "figures/CLUSTERS/friday_home_k-7.pdf", log = T)
save_clustering_iterations(sessions_saturday_city, k=6, it=6, filename = "figures/CLUSTERS/saturday_city_k-6.pdf", log = T)
save_clustering_iterations(sessions_saturday_city, k=7, it=6, filename = "figures/CLUSTERS/saturday_city_k-7.pdf", log = T)
save_clustering_iterations(sessions_saturday_home, k=6, it=6, filename = "figures/CLUSTERS/saturday_home_k-6.pdf", log = T)
save_clustering_iterations(sessions_saturday_home, k=7, it=6, filename = "figures/CLUSTERS/saturday_home_k-7.pdf", log = T)
save_clustering_iterations(sessions_sunday_city, k=7, it=6, filename = "figures/CLUSTERS/sunday_city_k-7.pdf", log = T)
save_clustering_iterations(sessions_sunday_home, k=6, it=6, filename = "figures/CLUSTERS/sunday_home_k-6.pdf", log = T)
save_clustering_iterations(sessions_sunday_home, k=7, it=6, filename = "figures/CLUSTERS/sunday_home_k-7.pdf", log = T)
save_clustering_iterations(sessions_sunday_home, k=8, it=6, filename = "figures/CLUSTERS/sunday_home_k-8.pdf", log = T)

save_clustering_iterations(sessions_workday_city, k=6, it=6, filename = "figures/CLUSTERS/workday_city_k-6_2.pdf", log = T)
save_clustering_iterations(sessions_friday_city, k=7, it=6, filename = "figures/CLUSTERS/friday_city_k-7.pdf", log = T)



save_clustering_iterations(sessions_friday_home, k=5, it=6, filename = "figures/friday_home_k-5__2.pdf", log = T)
save_clustering_iterations(sessions_friday_home, k=6, it=6, filename = "figures/friday_home_k-6__2.pdf", log = T)
save_clustering_iterations(sessions_saturday_city, k=4, it=6, filename = "figures/saturday_city_k-4__2.pdf", log = T)
save_clustering_iterations(sessions_saturday_city, k=5, it=6, filename = "figures/saturday_city_k-5__2.pdf", log = T)
save_clustering_iterations(sessions_saturday_city, k=6, it=6, filename = "figures/saturday_city_k-6__2.pdf", log = T)
save_clustering_iterations(sessions_sunday_city, k=4, it=6, filename = "figures/sunday_city_k-4__2.pdf", log = T)
save_clustering_iterations(sessions_sunday_city, k=5, it=6, filename = "figures/sunday_city_k-5__2.pdf", log = T)
save_clustering_iterations(sessions_sunday_city, k=6, it=6, filename = "figures/sunday_city_k-6__2.pdf", log = T)
```

For this study case, each data sub-set has been clustered 6 times and the optimal seeds for each sub-set have resulted as follows:

- Sessions workday city: `seed = 91`
- Sessions workday home: `seed = 643`
- Sessions friday city: `seed = 311`
- Sessions friday home: `seed = 436`
- Sessions saturday city: `seed = 668`
- Sessions saturday home: `seed = 537`
- Sessions sunday city: `seed = 908`
- Sessions sunday home: `seed = 566`


## Clustering

Finally, we can cluster and plot each sub-set with function `cluster_sessions()`, specifying the parameters `k` and `seed` with the corresponding values previously found:

```{r clustering, eval = F}
workday_city_GMM <- cluster_sessions(sessions_workday_city, k = 6, seed = 91, log = T)
workday_home_GMM <- cluster_sessions(sessions_workday_home, k = 6, seed = 643, log = T)
friday_city_GMM <- cluster_sessions(sessions_friday_city, k = 6, seed = 311, log = T)
friday_home_GMM <- cluster_sessions(sessions_friday_home, k = 6, seed = 436, log = T)
saturday_city_GMM <- cluster_sessions(sessions_saturday_city, k = 5, seed = 668, log = T)
saturday_home_GMM <- cluster_sessions(sessions_saturday_home, k = 5, seed = 537, log = T)
sunday_city_GMM <- cluster_sessions(sessions_sunday_city, k = 5, seed = 908, log = T)
sunday_home_GMM <- cluster_sessions(sessions_sunday_home, k = 6, seed = 566, log = T)
```

The object returned by `cluster_sessions()` function is a list with two other objects:

- sessions: data set with extra column `Cluster`, with the corresponding cluster number
- models: a `tibble` with the means and co-variance matrix of each cluster's Gaussian Mixture Models

These two objects correspond to the parameters `sessions` and `models` of the function `plot_bivarGMM()`, which plots each cluster as an ellipse over the sessions' points. Using `{purrr}` iteration we can generate a list with a plot for each sub-set.

```{r clusters plots}
bivarGMM_plots <- purrr::map2(
  list(workday_city_GMM$sessions, workday_home_GMM$sessions, friday_city_GMM$sessions, friday_home_GMM$sessions,
       saturday_city_GMM$sessions, saturday_home_GMM$sessions, sunday_city_GMM$sessions, sunday_home_GMM$sessions),
  list(workday_city_GMM$models, workday_home_GMM$models, friday_city_GMM$models, friday_home_GMM$models,
       saturday_city_GMM$models, saturday_home_GMM$models, sunday_city_GMM$models, sunday_home_GMM$models),
  ~ plot_bivarGMM(
    .x, .y, profiles_names = paste0(.y$cluster, " (", round(.y$ratio*100), "%)"), log = T, legend_nrow = 1
  )
)
```

```{r clusters plots save, eval = F, echo = F}
ggsave(
  filename = 'figures/CLUSTERS/clusters_GMM.pdf', 
  plot = cowplot::plot_grid(plotlist = bivarGMM_plots, nrow = 4, ncol = 2),
  width = 315, height = 445, units = "mm"
)
```

We will visualize the clusters of each subsets in Profiling section, together with the interpretation of each cluster and the corresponding assignation to a user profile.

```{r, eval=F, echo=F}
save(
  workday_city_GMM, workday_home_GMM, friday_city_GMM, friday_home_GMM, 
  saturday_city_GMM, saturday_home_GMM, sunday_city_GMM, sunday_home_GMM,
  file = 'arnhem_sessions_clusters.RData'
)
```

# Profiling

Clusters obtained from GMM don't give a lot of information themselves and separately may have an unclear meaning. In this section we will define each cluster to give them a meaning and relate them to generic user behaviors, i.e. user profiles. Moreover, not every clusters must correspond to a single user profile, clusters with the same or a similar meaning can be grouped to a user profile. Thus, the combination of these multiple Gaussian Mixture Models into a single user profile is what we expect to result in a daily generic behavior of EV users.

As a tool to define the different clusters, function `define_clusters()` prints the average value of the connection start time and connection duration (i.e. the centroid) of each cluster. If the cluster process was performed in logarithmic scale, these values are transformed to natural scale for a better understanding. Moreover, we can pass to the parameters `interpretations` and `profile_names` a list of character strings with the corresponding interpretation of the centroids (e.g. "Connection after work-time, leaving always next morning") and the user profile name assigned to each interpretation (e.g. "Commuter").

## Workdays

### Workday city

```{r, echo=F}
bivarGMM_plots[[1]]
```

```{r workday city profiling, eval = F, echo = F}
# Print centroids in natural scale
define_clusters(workday_city_GMM$models, log = T)

# Plot clusters
plot_points(workday_city_GMM$sessions, start = 6, log = FALSE, aes(color = Cluster))
```



```{r}
# Define clusters
workday_city_clusters_profiles <- define_clusters(
  models = workday_city_GMM$models,
  interpretations = c(
    "Short visits during the evening",
    "Short visits during the day",
    "Full-day working time",
    "Visits during the afternoon",
    "Visits during the morning or whole day",
    "Dinner time"
  ),
  profile_names = c(
    "Shortstay",
    "Shortstay",
    "Worktime",
    "Visit",
    "Visit",
    "Dinner"
  ),
  log = T
)

workday_city_clusters_profiles %>% 
  knitr::kable(digits = 2, col.names = c(
    "Cluster", "Controid Start time", "Centroid Connection hours", "Interpretation", "Profile"
  ))
```

### Workday home

```{r, echo=F}
bivarGMM_plots[[2]]
```

```{r workday home profiling, eval = F, echo = F}
# Print centroids in natural scale
define_clusters(workday_home_GMM$models, log = T)

# Plot clusters
plot_points(workday_home_GMM$sessions, start = 6, log = FALSE, aes(color = Cluster)) 
```

```{r}
# Define clusters
workday_home_clusters_profiles <- define_clusters(
  models = workday_home_GMM$models,
  interpretations = c(
    "Connection during the afternoon, not necessarily leaving next morning",
    "Connection after work, leaving next morning",
    "Connection during the night, leaving next morning",
    "Connection after work, leaving next morning",
    "Connection during the night, not necessarily leaving next morning",
    "Connection during the afternoon, leaving next morning"
  ),
  profile_names = c(
    "Home",
    "Commuters",
    "Pillow",
    "Commuters",
    "Pillow",
    "Home"
  ),
  log = T
)

workday_home_clusters_profiles %>% 
  knitr::kable(digits = 2, col.names = c(
    "Cluster", "Controid Start time", "Centroid Connection hours", "Interpretation", "Profile"
  ))
```


## Fridays

### Friday city

```{r, echo=F}
bivarGMM_plots[[3]]
```

```{r friday city profiling, eval = F, echo = F}
# Print centroids in natural scale
define_clusters(friday_city_GMM$models, log = TRUE)

# Plot clusters
plot_points(friday_city_GMM$sessions, start = 6, log = F, aes(color = Cluster)) 
```

```{r}
# Define clusters
friday_city_clusters_profiles <- define_clusters(
  models = friday_city_GMM$models,
  interpretations = c(
    "Dinner time",
    "Full-day working time",
    "Short visits during the evening",
    "Visits during the afternoon",
    "Visits during the morning or whole day",
    "Short visits during the day"
  ),
  profile_names = c(
    "Dinner",
    "Worktime",
    "Shortstay",
    "Visit",
    "Visit",
    "Shortstay"
  ),
  log = T
)

friday_city_clusters_profiles %>% 
  knitr::kable(digits = 2, col.names = c(
    "Cluster", "Controid Start time", "Centroid Connection hours", "Interpretation", "Profile"
  ))
```

### Friday home

```{r, echo=F}
bivarGMM_plots[[4]]
```

```{r friday home profiling, eval = F, echo = F}
# Print centroids in natural scale
define_clusters(friday_home_GMM$models, log = T)

# Plot clusters
plot_points(friday_home_GMM$sessions, start = 6, log = F, aes(color = Cluster))
```

```{r}
# Define clusters
friday_home_clusters_profiles <- define_clusters(
  models = friday_home_GMM$models,
  interpretations = c(
    "Connection during the night, not necessarily leaving next morning",
    "Connection during the night, leaving during next morning",
    "Connection after work, leaving during next morning",
    "Connection during the night, not necessarily leaving next morning",
    "Connection during the afternoon, leaving next morning",
    "Connection during the afternoon, not necessarily leaving next morning"
  ),
  profile_names = c(
    "Pillow",
    "Pillow",
    "Commuters",
    "Pillow",
    "Home",
    "Home"
  ),
  log = T
)

friday_home_clusters_profiles %>% 
  knitr::kable(digits = 2, col.names = c(
  "Cluster", "Controid Start time", "Centroid Connection hours", "Interpretation", "Profile"
  ))
```

Notes:

* The Friday Home cluster that leave next day is not so narrow (concentrated) than Workday Home cluster since people has no timetable on weekends morning.
* The Commuter profile on Fridays is different than Workdays Commuter since the after-work sessions are more dispersed in terms of both starting time and duration. This could be a result of not working on Friday afternoon (or leave early from work) and not having to leave early next morning.


## Saturdays 

### Saturday city

```{r, echo=F}
bivarGMM_plots[[5]]
```

```{r saturday city profiling, eval = F, echo = F}
# Print centroids in natural scale
define_clusters(saturday_city_GMM$models, log = T)

# Plot clusters
plot_points(saturday_city_GMM$sessions, start = 6, log = F, aes(color = Cluster))
```

```{r}
# Define clusters
saturday_city_clusters_profiles <- define_clusters(
  models = saturday_city_GMM$models,
  interpretations = c(
    "Visits during the morning",
    "Short visits during the day",
    "Dinner time",
    "Visits during the day",
    "Visits during the afternoon"
  ),
  profile_names = c(
    "Visit",
    "Shortstay",
    "Dinner",
    "Visit",
    "Visit"
  ),
  log = T
)

saturday_city_clusters_profiles %>% 
  knitr::kable(digits = 2, col.names = c(
    "Cluster", "Controid Start time", "Centroid Connection hours", "Interpretation", "Profile"
  ))
```

Notes:

* Shortstay profile is in minority during Saturdays in favour to Visit profile, probably because people have time to make longer visits rather than short ones.


## Saturday home

```{r, echo=F}
bivarGMM_plots[[6]]
```

```{r saturday home profiling, eval = F, echo = F}
# Print centroids in natural scale
define_clusters(saturday_home_GMM$models, log = T)

# Plot clusters
plot_points(saturday_home_GMM$sessions, start = 6, log = F, aes(color = Cluster))
```

```{r}
# Define clusters
saturday_home_clusters_profiles <- define_clusters(
  models = saturday_home_GMM$models,
  interpretations = c(
    "Connection during the early-afternoon, leaving next morning",
    "Connection during the afternoon, not necessarily leaving next morning",
    "Connection during the night, not necessarily leaving next morning",
    "Connection during the afternoon, leaving next morning",
    "Connection during the night, leaving next morning"
  ),
  profile_names = c(
    "Home", 
    "Home", 
    "Pillow",
    "Home",
    "Pillow"
  ),
  log = T
)

saturday_home_clusters_profiles %>% 
  knitr::kable(digits = 2, col.names = c(
    "Cluster", "Controid Start time", "Centroid Connection hours", "Interpretation", "Profile"
  ))
```

Notes:

* The Friday Home cluster that leave next day is not so narrow (concentrated) than Workday Home cluster since people has no timetable on weekends morning. The weekends user profiles are more variable and less clear.
* On weekends we don't have commuters since they don't go from work to home, thus the afternoon sessions belong now to Home profile.


## Sundays 

### Sunday city

```{r, echo=F}
bivarGMM_plots[[7]]
```

```{r sunday city profiling, eval = F, echo = F}
# Print centroids in natural scale
define_clusters(sunday_city_GMM$models, log = T)

# Plot clusters
plot_points(sunday_city_GMM$sessions, start = 6, log = F, aes(color = Cluster))
```

```{r}
# Define clusters
sunday_city_clusters_profiles <- define_clusters(
  models = sunday_city_GMM$models,
  interpretations = c(
    "Visits during the morning",
    "Short visits during the day",
    "Visits during the morning or whole day",
    "Visits during the day",
    "Dinner time"
  ),
  profile_names = c(
    "Visit",
    "Shortstay",
    "Visit",
    "Visit",
    "Dinner"
  ),
  log = T
)

sunday_city_clusters_profiles %>% 
  knitr::kable(digits = 2, col.names = c(
    "Cluster", "Controid Start time", "Centroid Connection hours", "Interpretation", "Profile"
  ))
```

Notes:

* Shortstay visits are less relevant during the Sunday since the shops or places to make short assignments are closed on this weekday.
* The Dinner profile has more weight here (20%) than the other time-cycles (around 15%)


## Sunday home

```{r, echo=F}
bivarGMM_plots[[8]]
```

```{r sunday home profiling, eval = F, echo = F}
# Print centroids in natural scale
define_clusters(sunday_home_GMM$models, log = T)

# Plot clusters
plot_points(sunday_home_GMM$sessions, start = 6, log = F, aes(color = Cluster))
```

```{r}
# Define clusters
sunday_home_clusters_profiles <- define_clusters(
  models = sunday_home_GMM$models,
  interpretations = c(
    "Connection during the afternoon, leaving next morning",
    "Connection during the night, not necessarily leaving next morning",
    "Connection during the night, leaving next morning",
    "Connection during the afternoon, leaving next morning",
    "Connection during the afternoon, leaving next morning",
    "Connection during the afternoon, not necessarily leaving next morning"
  ),
  profile_names = c(
    "Home",
    "Pillow",
    "Pillow",
    "Home",
    "Home",
    "Home"
  ),
  log = T
)

sunday_home_clusters_profiles %>% 
  knitr::kable(digits = 2, col.names = c(
    "Cluster", "Controid Start time", "Centroid Connection hours", "Interpretation", "Profile"
  ))
```

Notes:

* Here we have again a narrow afternoon Home clusters that show people leaving next morning since Monday is a working day.
* On weekends we don't have commuters since they don't go from work to home.


# Sessions classification into user profiles

After assigning a user profile to each cluster through the clusters definitions with function `define_clusters()`, we can use the data frame that this function outputs as the `clusters_definition` parameter of function `set_profiles()`. This function wraps all sub-sets sessions and clusters definitions to return a total sessions data set with an extra `Profile` column, finishing with this function the user profile classification of the charging sessions data set.
The other parameter that function `set_profiles()` needs is the `sessions_clustered`, which is the `sessions` object of the output from `cluster_sessions` function.

```{r sessions result}
# Join the classification of each subset
sessions_profiles <- set_profiles(
  sessions_clustered = list(
    workday_city_GMM$sessions, workday_home_GMM$sessions, friday_city_GMM$sessions, friday_home_GMM$sessions,
    saturday_city_GMM$sessions, saturday_home_GMM$sessions, sunday_city_GMM$sessions, sunday_home_GMM$sessions
  ),
  clusters_definition = list(
    workday_city_clusters_profiles, workday_home_clusters_profiles, friday_city_clusters_profiles, friday_home_clusters_profiles,
    saturday_city_clusters_profiles, saturday_home_clusters_profiles, sunday_city_clusters_profiles, sunday_home_clusters_profiles
  )
)

head(sessions_profiles)
```

To visualize the classification we can plot the charging sessions points with a different color for every user profile, and separate between time-cycle.

```{r plot classification profiles}
classification_profiles_plot <- plot_points(sessions_profiles, start = 6, log = FALSE, aes(color = Profile), size = 0.3) + 
  facet_wrap(~ Timecycle)

classification_profiles_plot
```

Although the same user profiles names are used in all time-cycles, we can appreciate some differences that support our approach to make a model for each time-cycle independently:

* Worktime and Commuter sessions appear with a clear shape only during working days.
* Short-stay sessions are in minority during Weekends days since we have longer sessions instead, or shops are closed.
* The Home profile present narrower cluster shapes when the next day is a working day (Workday and Sunday time-cycles). When users don't have a defined timetable the following day, sessions are more variable in terms of both duration and start time.
* Dinner profile has more importance during Sundays than the other time-cycles.

To conclude the profiling section, we could summarize the user profiles with the following classification based on their connections features:

* **Specific profiles**: all sessions have similar start time *and* connection duration
    - **Worktime**: connections start between 8:00 and 9:00, with a duration of 8-9 hours.
    - **Commuters**: connecitons start between 18:00 and 19:00, with a duration of 13-14 hours (leaving between 7:00-8:00 of next day)
    - **Diner**: connections start between 18:00 and 19:00, with a duration of 3-4 hours.
  
* **Semi-specific profiles**: sessions have similar start time *or* connection duration.
    - **Shortstay**: connection duration of 1.5 hours in average.
    - **Pillow**: connection start time from 21:00. 
  
* **Variable profiles**: sessions with a wide range of start time and connection duration values
    - **Visit**: city sessions that start during all day with a variety of short and long connection duration.
    - **Home**: home sessions that start during all afternoon not necessarily leaving next day.


<!-- * Long worktime sessions during Workday (4 days working week) -->
<!-- * Commuters = connect afterwork, leave next day -> Not on Saturdays (should be a minoritary profile) -->

```{r plot classification profiles save, eval = F, echo = F}
ggsave(
  filename = 'figures/CLUSTERS/classification_profiles.pdf', plot = classification_profiles_plot, paper="a4r", width = 40, height = 10
)
```

```{r plot classification clusters, eval = F, echo = F}
classification_clusters_plot <- plot_points(sessions_profiles, start = 6, log = FALSE, aes(color = Cluster), size = 0.3) + 
  facet_wrap(~ Timecycle)

ggsave(
  filename = 'figures/CLUSTERS/classification_clusters.pdf', plot = classification_clusters_plot, paper="a4r", width = 40, height = 10
)
```

```{r, eval=F, echo=F}
save.image('arnhem_sessions_profiles.RData')
```


# User profiles modeling

As seen before, we have divided the data in two subsets considering the time-cycles where EV user profiles change their behaviors. In this case we have four different time cycles: Workdays, Fridays, Saturdays and Sundays.

Each time-cycle will have its own connection models (to estimate connection start time and duration) and energy models (to estimate energy charged) for every user profile.

Each time-cycle model will be stored in `tibbles` with the following variables:

- `profile`: Character vector with profiles names. Each profile is a row in the tibble.
- `profile_ratio`: Numeric vector with the ratio or percentage of sessions corresponding to each profile for this time cycle, obtained from function `get_connection_models()`. In case of modifying these ratios, their values must be always between 0 and 1.
- `connection_models`: List of tibbles containing the connection models of each profile, obtained from function `get_connection_models()`.
- `energy_models`: List of tibbles containing the energy models of each profile, obtained from function `get_energy_models()`.

The energy consumed is very sensitive to latest models of EV in the market, since the batteries are larger with every new model. To deal with this variability, we will make the energy model of each profile and time-cycle based on January and February of 2020.

```{r sessions_energy}
sessions_energy_models <- sessions_profiles %>% 
  filter(year(ConnectionStartDateTime) == 2020, month(ConnectionStartDateTime) < 3)
```

On one hand, after obtaining the connection models (i.e. bi-variate Gaussian Mixture Models weights, means and co-variance matrices) with function `get_connection_models()`, we can visualize them with function `plot_model_clusters()` which outputs a similar ellipses plot than function `plot_bivarGMM()` but using a different color for each user profile instead of clusters (the clusters of a same profile have the same color now). 
On the other hand, the energy models (i.e. uni-variate Gaussian Mixture Models weigths, means and variance) obtained with function `get_energy_models()` can be visualized with function `plot_energy_models_density()`. This function simulates random energy values, makes the density curve and overlaps the simulated density curve with the real density curve of the user profile's energy values. This is useful to appreciate how the modeled values fit the real ones and increase or decrease the number of Gaussian components.

## Workdays models

Connection models: Bi-variate Gaussian Mixture Models

```{r workday connection models, eval=T}
# Build the models
workday_connection_models <- get_connection_models(
  list(workday_city_GMM, workday_home_GMM),
  list(workday_city_clusters_profiles, workday_home_clusters_profiles)
)
```
```{r}
# Plot the bivariate GMM
workday_connection_models_plot <- plot_model_clusters(
  list(workday_city_GMM, workday_home_GMM),
  list(workday_city_clusters_profiles, workday_home_clusters_profiles),
  workday_connection_models[c("profile", "ratio")]
)
```
```{r, echo=F, eval=T}
workday_connection_models_plot
```
```{r, echo=F, eval=F}
# Save the result
ggsave(
  filename = 'figures/MODELS/connection_workday_GMM.pdf', 
  plot = workday_connection_models_plot,
  paper="a4r", width = 40, height = 20
)
```

Energy models: Uni-variate Gaussian Mixture Models

```{r workday energy models, eval=F}
# Build the models
set.seed(1)
workday_energy_models <- sessions_energy_models %>% 
  filter(Timecycle == 'Workday') %>% 
  get_energy_models(
    c("Visit"=6, "Worktime"=6, "Dinner"=6, "Shortstay"=6, "Home"=6, "Commuters"=6, "Pillow"=6),
    log = TRUE
  )
```
```{r}
# Plot the univariate GMM
workday_energy_models_plots <- sessions_energy_models %>% 
  filter(Timecycle == 'Workday') %>% 
  plot_energy_models_density(workday_energy_models, log = T)
```
```{r, echo=F, eval=T}
workday_energy_models_plots
```
```{r, echo=F, eval=F}
# Save the result
ggsave(
  filename = 'figures/MODELS/energy_workday_GMM.pdf', 
  plot = workday_energy_models_plots,
  paper="a4r", width = 40, height = 20
)
```


## Fridays models

Connection models: Bi-variate Gaussian Mixture Models

```{r friday connection models, eval=F}
# Build the models
friday_connection_models <- get_connection_models(
  list(friday_city_GMM, friday_home_GMM),
  list(friday_city_clusters_profiles, friday_home_clusters_profiles)
)
```
```{r}
# Plot the bivariate GMM
friday_connection_models_plot <- plot_model_clusters(
  list(friday_city_GMM, friday_home_GMM),
  list(friday_city_clusters_profiles, friday_home_clusters_profiles),
  friday_connection_models[c("profile", "ratio")]
)
```
```{r, echo=F, eval=T}
friday_connection_models_plot
```
```{r, echo=F, eval=F}
# Save the result
ggsave(
  filename = 'figures/MODELS/connection_friday_GMM.pdf', 
  plot = friday_connection_models_plot,
  paper="a4r", width = 40, height = 20
)
```

Energy models: Uni-variate Gaussian Mixture Models

```{r friday energy models, eval=F}
# Build the models
set.seed(4)
friday_energy_models <- sessions_energy_models %>% 
  filter(Timecycle == 'Friday') %>% 
  get_energy_models(
    c("Visit"=6, "Worktime"=6, "Dinner"=6, "Shortstay"=6, "Home"=6, "Commuters"=6, "Pillow"=6),
    log = TRUE
  )
```
```{r}
# Plot the univariate GMM
friday_energy_models_plots <- sessions_energy_models %>% 
  filter(Timecycle == 'Friday') %>% 
  plot_energy_models_density(friday_energy_models, log = T)
```
```{r, echo=F, eval=T}
friday_energy_models_plots
```
```{r, echo=F, eval=F}
# Save the result
ggsave(
  filename = 'figures/MODELS/energy_friday_GMM.pdf', 
  plot = friday_energy_models_plots,
  paper="a4r", width = 40, height = 20
)
```


## Saturdays models

Connection models: Bi-variate Gaussian Mixture Models

```{r saturday connection models, eval=F}
# Build the models
saturday_connection_models <- get_connection_models(
  list(saturday_city_GMM, saturday_home_GMM),
  list(saturday_city_clusters_profiles, saturday_home_clusters_profiles)
)
```
```{r}
# Plot the bivariate GMM
saturday_connection_models_plot <- plot_model_clusters(
  list(saturday_city_GMM, saturday_home_GMM),
  list(saturday_city_clusters_profiles, saturday_home_clusters_profiles),
  saturday_connection_models[c("profile", "ratio")]
)
```
```{r, echo=F, eval=T}
saturday_connection_models_plot
```
```{r, echo=F, eval=F}
# Save the result
ggsave(
  filename = 'figures/MODELS/connection_saturday_GMM.pdf', 
  plot = saturday_connection_models_plot,
  paper="a4r", width = 40, height = 20
)
```

Energy models: Uni-variate Gaussian Mixture Models

```{r saturday energy models, eval=F}
# Build the models
set.seed(1)
saturday_energy_models <- sessions_energy_models %>% 
  filter(Timecycle == 'Saturday') %>% 
  get_energy_models(
    c("Visit"=6, "Dinner"=6, "Shortstay"=6, "Home"=6, "Pillow"=6),
    log = TRUE
  )
```
```{r}
# Plot the univariate GMM
saturday_energy_models_plots <- sessions_energy_models %>% 
  filter(Timecycle == 'Saturday') %>% 
  plot_energy_models_density(saturday_energy_models, log = T)
```
```{r, echo=F, eval=T}
saturday_energy_models_plots
```
```{r, echo=F, eval=F}
# Save the result
ggsave(
  filename = 'figures/MODELS/energy_saturday_GMM.pdf', 
  plot = saturday_energy_models_plots,
  paper="a4r", width = 40, height = 20
)
```


## Sundays models

Connection models: Bi-variate Gaussian Mixture Models

```{r sunday connection models, eval=F}
# Build the models
sunday_connection_models <- get_connection_models(
  list(sunday_city_GMM, sunday_home_GMM),
  list(sunday_city_clusters_profiles, sunday_home_clusters_profiles)
)
```
```{r}
# Plot the bivariate GMM
sunday_connection_models_plot <- plot_model_clusters(
  list(sunday_city_GMM, sunday_home_GMM),
  list(sunday_city_clusters_profiles, sunday_home_clusters_profiles),
  sunday_connection_models[c("profile", "ratio")]
)
```
```{r, echo=F, eval=T}
sunday_connection_models_plot
```
```{r, echo=F, eval=F}
# Save the result
ggsave(
  filename = 'figures/MODELS/connection_sunday_GMM.pdf', 
  plot = sunday_connection_models_plot,
  paper="a4r", width = 40, height = 20
)
```

Energy models: Uni-variate Gaussian Mixture Models

```{r sunday energy models, eval=F}
# Build the models
set.seed(15)
sunday_energy_models <- sessions_energy_models %>% 
  filter(Timecycle == 'Sunday') %>% 
  get_energy_models(
    c("Visit"=6, "Dinner"=6, "Shortstay"=6, "Home"=6, "Pillow"=6),
    log = TRUE
  )
```
```{r}
# Plot the univariate GMM
sunday_energy_models_plots <- sessions_energy_models %>% 
  filter(Timecycle == 'Sunday') %>% 
  plot_energy_models_density(sunday_energy_models, log = T)
```
```{r, echo=F, eval=T}
sunday_energy_models_plots
```
```{r, echo=F, eval=F}
# Save the result
ggsave(
  filename = 'figures/MODELS/energy_sunday_GMM.pdf', 
  plot = sunday_energy_models_plots,
  paper="a4r", width = 40, height = 20
)
```

```{r, eval=F, echo=F}
save(
  workday_connection_models, workday_energy_models, friday_connection_models, friday_energy_models, 
  saturday_connection_models, saturday_energy_models, sunday_connection_models, sunday_energy_models,
  file = 'arnhem_sessions_models.RData'
)
```

## Save the EV models

The package `{evprof}` proposes a standard format for the EV model with the function `get_ev_model()`. This function returns an object of class `evmodel`, which can be used then as input of `{evsim}` package to simulate new sessions. The function requires the following variables:

- `names`: Character vector with the names of the time cycle
- `months_lst`: List of numeric vectors, each observation containing the months of validity of each time cycle
- `wdays_lst`: List of numeric vectors, each observation containing the weekdays of validity (week start = 1) of each time cycle 
- `connection_GMM`: List of the connection models returned by function `get_connection_models()`
- `energy_GMM`List of the connection models returned by function `get_energy_models()`
- `connection_log`: logical `TRUE` since we have built the connection models in a logarithmic scale
- `energy_log`: logical `TRUE` since we have built the energy models in a logarithmic scale
- `tzone`: In our case the sessions are in `Europe/Amsterdam` time-zoneâ™£


```{r total models}
ev_model <- get_ev_model(
  names = c('Workday', 'Friday', 'Saturday', 'Sunday'), 
  months_lst = list(1:12), 
  wdays_lst = list(1:4, 5, 6, 7),
  connection_GMM = list(
    workday_connection_models, friday_connection_models, 
    saturday_connection_models, sunday_connection_models
  ),
  energy_GMM = list(
    workday_energy_models, friday_energy_models, 
    saturday_energy_models, sunday_energy_models
  ),
  connection_log = T,
  energy_log = T
)

ev_model
```

Then we can save the object to an `.RDS` file with:

```{r save model, eval=F}
save_ev_model(ev_model)
```


# Compare BAU and simulated demand

To simulate new sessions with our models we will make use of package `{evsim}`. We need the following data:

- Number of sessions of each model
- User profiles proportions for each model
- Charging powers proportions


We will compare the current demand with the demand obtained from our models for a certain period, for example the two first weeks of September 2019. The time series demand from a sessions data set can also be obtained with `{evsim}` package, using function `get_demand()`.

```{r bau demand, eval=F}
library(dygraphs)
library(lubridate)
library(evsim)
library(dutils)

interval_mins <- 15
start_date <- dmy(01022020) %>% as_datetime(tz = getOption('evprof.tzone')) %>% floor_date('day')
end_date <- dmy(29022020) %>% as_datetime(tz = getOption('evprof.tzone')) %>% floor_date('day')
dttm_seq <- seq.POSIXt(from = start_date, to = end_date, by = paste(interval_mins, 'min'))

sessions_demand <- sessions_profiles %>% 
  filter(between(ConnectionStartDateTime, start_date, end_date))

demand <- sessions_demand %>% 
  get_demand(dttm_seq)
```

```{r bau demand plot, fig.width = 9}
demand %>% dyplot(fillGraph = T)
```


To simulate an equivalent type of sessions we have to find the following parameters:

* **Charging rates distribution**: We can get the current charging power distribution with function `get_charging_rates_distribution()`:

```{r charging power distribution}
charging_rates <- get_charging_rates_distribution(sessions_demand) %>% 
  select(power, ratio) %>% 
  mutate(power = c(3.7, 7.3, 11, 22))

print(charging_rates)
```

* **Number of sessions per day**: The daily number of sessions for each model

```{r daily number of sessions}
n_sessions <- sessions_demand %>% 
  group_by(Timecycle) %>% 
  summarise(n = n()) %>% 
  mutate(n_day = round(n/c(16, 4, 4, 4))) %>% # Divided by the monthly days of each time-cycle
  select(time_cycle = Timecycle, n_sessions = n_day)

print(n_sessions)
```

* **Profiles distribution**: The user profiles proportion for each model

```{r profiles ratios}
profiles_ratios <- sessions_demand %>% 
  group_by(Timecycle, Profile) %>% 
  summarise(n = n()) %>% 
  mutate(ratio = n/sum(n)) %>% 
  select(time_cycle = Timecycle, profile = Profile, ratio)

head(profiles_ratios, 10)
```


The function `simulate_sessions()` of package `{evsim}` lets to simulate the sessions with the configuration above, together with the `dates` of the simulated sessions and the `interval_mins` as the times resolution (in minutes). Moreover, parameters `connection_log` and `energy_log` must be TRUE if the models have been built in a logarithmic scale.

```{r estimate sessions, eval=F}
my_ev_model <- ev_model %>% 
  update_profiles_ratios(
    profiles_ratios # Update the distribution of user profiles
  )

sessions_estimated <- simulate_sessions(
  my_ev_model,
  n_sessions,
  charging_rates, 
  dates = unique(date(dttm_seq)), 
  interval_mins
)
```
```{r estimated sessions, eval=T, echo=F}
# Check that the charging rates distribution is correct with:
# sessions_estimated %>% get_charging_rates_distribution()

head(sessions_estimated)
```

Finally, we can calculate the estimated demand and compare it with the real demand:

```{r simualted demand, eval=F}
estimated_demand <- sessions_estimated %>% 
  get_demand(dttm_seq)
  
comparison_demand <- tibble(
  datetime = dttm_seq,
  demand_real = rowSums(demand[-1]),
  demand_estimated = rowSums(estimated_demand[-1])
) 
```

```{r, eval=F, echo=F}
save(ev_model, my_ev_model, sessions_demand, sessions_estimated, demand, estimated_demand, comparison_demand, file = 'arnhem_comparison_demand.RData')
```

```{r simualted demand plot, fig.width = 9}
comparison_demand %>% 
  dyplot(ylab = 'kW') %>% 
  dySeries('demand_real', 'Real demand', color = 'black', strokePattern = 'dashed', strokeWidth = 2) %>% 
  dySeries('demand_estimated', 'Estimated demand', color = 'navy', fillGraph = T)
```

It is obvious that the two demand curves don't match perfectly because we are not doing a forecasting model but a simulation model, without inputs that conditionate a particular output. What we can compare is the day times where the demand peaks or valleys occur, and this is a positive comparison for our models, which coincide almost perfectly with the real demand. Moreover, the peak values are in general in concordance with the real peaks.




